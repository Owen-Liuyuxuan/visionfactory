{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "from onnxruntime.tools import pytorch_export_contrib_ops\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import deform_conv2d_onnx_exporter\n",
    "import numpy as np\n",
    "deform_conv2d_onnx_exporter.register_deform_conv2d_onnx_op()\n",
    "pytorch_export_contrib_ops.register()\n",
    "import sys\n",
    "import os\n",
    "\n",
    "package_path = os.path.dirname(sys.path[0])  #two folders upwards\n",
    "sys.path.insert(0, package_path)\n",
    "\n",
    "from vision_base.utils.builder import build\n",
    "from vision_base.utils.utils import cfg_from_file\n",
    "from vision_base.networks.utils.utils import load_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfg_from_file('../configs/res34_monodepth_odaiba.py')\n",
    "# checkpoint_path = \"../odaiba_single/checkpoint/monodepth.networks.models.meta_archs.monodepth2_model.MonoDepthWPose_latest.pth\"\n",
    "checkpoint_path = \"../workdirs/odaiba_single/checkpoint/monodepth.networks.models.meta_archs.monodepth2_model.MonoDepthWPose_latest.pth\"\n",
    "onnx_output = \"merge_multicam_monodepth_smaller.onnx\"\n",
    "is_export_rgb = True\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ../workdirs/odaiba_single/checkpoint/monodepth.networks.models.meta_archs.monodepth2_model.MonoDepthWPose_latest.pth.\n"
     ]
    }
   ],
   "source": [
    " # Force GPU selection in command line\n",
    "cfg.trainer.gpu = gpu\n",
    "torch.cuda.set_device(cfg.trainer.gpu)\n",
    "\n",
    "# Create the model\n",
    "meta_arch = build(**cfg.meta_arch)\n",
    "meta_arch = meta_arch.cuda()\n",
    "\n",
    "load_models(checkpoint_path, meta_arch, map_location=f'cuda:{gpu}', strict=False)\n",
    "meta_arch.eval()\n",
    "print(f\"Loaded model from {checkpoint_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDepthExportModel(torch.nn.Module):\n",
    "    def __init__(self, meta_arch, cfg, is_export_rgb=True):\n",
    "        super().__init__()\n",
    "        self.meta_arch = meta_arch\n",
    "        self.is_export_rgb = is_export_rgb\n",
    "        self.register_buffer(\"rgb_mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).cuda())\n",
    "        self.register_buffer(\"rgb_std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).cuda())\n",
    "\n",
    "        w_range = np.arange(0, cfg.data.rgb_shape[1], dtype=np.float32)\n",
    "        h_range = np.arange(0, cfg.data.rgb_shape[0], dtype=np.float32)\n",
    "        w_grid, h_grid = np.meshgrid(w_range, h_range) #[H, W]\n",
    "        base_depth_image = np.stack([w_grid, h_grid,\n",
    "                                     np.ones_like(w_grid),\n",
    "                                     np.ones_like(w_grid)], axis=2)[...,np.newaxis][None] # [1, H, W, 4, 1]\n",
    "        self.register_buffer(\"base_depth_image\", torch.tensor(base_depth_image).cuda())\n",
    "\n",
    "        bottom_mask = np.zeros((cfg.data.rgb_shape[0], cfg.data.rgb_shape[1]), dtype=np.bool_)\n",
    "        bottom_mask[-(cfg.data.rgb_shape[0]//4):, :] = True\n",
    "        self.register_buffer(\"bottom_mask\", torch.tensor(bottom_mask).cuda())\n",
    "\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        image = image / 256.0\n",
    "        original_image = image.clone()\n",
    "        image = image - self.rgb_mean\n",
    "        image = image / self.rgb_std\n",
    "        return image, original_image\n",
    "\n",
    "    def depth_image_to_point_cloud_array(self, depth_image, K, T, rgb_image=None, mask=None):\n",
    "        \"\"\"  convert depth image into color pointclouds [xyzbgr]\n",
    "        depth_image: [B, 1, H, W] -> fully normalized\n",
    "        K: [B, 3, 4]\n",
    "        T: [B, 4, 4] -> camera to base_link / world\n",
    "        rgb_image: [B, 3, H, W] -> 0-1\n",
    "        mask: [B, H, W]\n",
    "        \"\"\"\n",
    "        P_expanded = torch.eye(4).cuda()\n",
    "        P_expanded = P_expanded[None].repeat([depth_image.shape[0], 1, 1]) #[B, 4, 4]\n",
    "        P_expanded[:, 0:3, :] = K\n",
    "        P_inv = torch.inverse(P_expanded) # [B, 4 ,4]\n",
    "\n",
    "        #[H, W, 4, 1]\n",
    "        base_depth_image = self.base_depth_image.repeat([depth_image.shape[0], 1, 1, 1, 1]).clone()\n",
    "        base_depth_image[:, :, :, 0:3, 0] = base_depth_image[:, :, :, 0:3, 0] * depth_image[:, 0, :, :, None]\n",
    "        # B, H, W, 3 * B, H, W, 1\n",
    "\n",
    "        # B, 1, 1, 4, 4 * B, H, W, 4, 1 -> B, H, W, 4, 1\n",
    "        pc_3d = torch.matmul(P_inv[:, None, None, ...], base_depth_image) #[B, H, W, 4, 1]\n",
    "\n",
    "        pc_3d = torch.matmul(T[:, None, None, ...], pc_3d)[..., 0:3, 0] # [B, H, W, 4, 1] -> [B, H, W, 3]\n",
    "\n",
    "        if self.is_export_rgb:\n",
    "            rgb_image = rgb_image.permute(0, 2, 3, 1).contiguous() #[B, 3, H, W] -> [B, H, W, 3]\n",
    "            pc_3d = torch.cat([pc_3d, rgb_image], dim=3) #[B, H, W, 6]\n",
    "\n",
    "        mask = torch.logical_and(mask > 0, depth_image[:, 0] < 60)\n",
    "        # mask = mask * (1 - bottom_mask.float())  # [B, H, W] * depth_image[:, 0] > 0\n",
    "        point_cloud = pc_3d[mask,:] # [N, 6]\n",
    "        \n",
    "        return point_cloud\n",
    "\n",
    "    def forward(self, image, P, T, masks):\n",
    "        image, original_image = self.normalize_image(image)\n",
    "        depths = self.meta_arch.dummy_forward(image, P)['depth'] # [B, 1, H, W]\n",
    "        point_cloud = self.depth_image_to_point_cloud_array(depths, P, T, original_image, masks)\n",
    "        return point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_depth_model = MultiDepthExportModel(meta_arch, cfg, is_export_rgb=is_export_rgb)\n",
    "multi_depth_model.eval()\n",
    "multi_depth_model.cuda()\n",
    "\n",
    "B = 2\n",
    "dummy_image = torch.zeros([B, cfg.data.rgb_shape[2], cfg.data.rgb_shape[0], cfg.data.rgb_shape[1]]).cuda()\n",
    "dummy_T = torch.eye(4).expand(B, -1, -1).cuda()\n",
    "dummy_P = dummy_T[:, 0:3, :].clone()\n",
    "dummy_masks = torch.ones([B, cfg.data.rgb_shape[0], cfg.data.rgb_shape[1]]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([720896, 6])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = multi_depth_model(dummy_image, dummy_P, dummy_T, dummy_masks)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: ONNX Preprocess - Removing mutation from node aten::copy_ on block input: 'P2'. This changes graph semantics.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of com.microsoft::Inverse type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of com.microsoft::Inverse type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of com.microsoft::Inverse type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "dummy_input = (dummy_image, dummy_P, dummy_T, dummy_masks)\n",
    "torch.onnx.export(multi_depth_model, dummy_input, onnx_output,\n",
    "                   input_names=['image', 'P2', 'T', 'masks'],\n",
    "                   output_names=['point_cloud'], opset_version=11,\n",
    "                    dynamic_axes={'image': {0: 'batch_size'}, 'P2': {0: 'batch_size'}, 'T': {0: 'batch_size'}, 'masks': {0: 'batch_size'}, 'point_cloud': {0: 'numbers'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 03:10:07.984826601 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-12-02 03:10:07.984863999 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual output of onnxruntime session: outputs[0].shape=(573440, 6)\n"
     ]
    }
   ],
   "source": [
    "ort_session = ort.InferenceSession(onnx_output, providers=[('CUDAExecutionProvider', {'device_id':gpu})])\n",
    "outputs = ort_session.run(None, {'image': dummy_image.cpu().numpy(), 'P2': dummy_P.cpu().numpy(), 'T': dummy_T.cpu().numpy(), 'masks': dummy_masks.cpu().numpy()})\n",
    "\n",
    "print(f\"The actual output of onnxruntime session: outputs[0].shape={outputs[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
